{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Installs\n",
    "!pip install torch torchvision torchaudio transformers pytorch-lightning sentencepiece Pillow --quiet\n",
    "!pip install albumentations[imgaug] --quiet\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, AdamW, get_scheduler\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and Hyperparameters\n",
    "\n",
    "data_root = \"/teamspace/studios/this_studio\"\n",
    "\n",
    "train_images_dir = os.path.join(data_root, 'train', 'images')\n",
    "train_annotations_dir = os.path.join(data_root, 'train', 'annotations')\n",
    "\n",
    "val_images_dir = os.path.join(data_root, 'val', 'images')\n",
    "val_annotations_dir = os.path.join(data_root, 'val', 'annotations')\n",
    "\n",
    "test_images_dir = os.path.join(data_root, 'test', 'images')\n",
    "test_annotations_dir = os.path.join(data_root, 'test', 'annotations')\n",
    "\n",
    "splits_dir = os.path.join(data_root, 'splits')\n",
    "train_split_file = os.path.join(splits_dir, 'train.txt')\n",
    "val_split_file = os.path.join(splits_dir, 'val.txt')\n",
    "test_split_file = os.path.join(splits_dir, 'test.txt')\n",
    "\n",
    "save_dir = os.path.join(data_root, 'donut_trained_model')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 3e-5\n",
    "weight_decay = 0.001\n",
    "max_epochs = 30\n",
    "warmup_ratio = 0.05\n",
    "batch_size = 2\n",
    "num_workers = 4\n",
    "max_length = 512\n",
    "precision = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Collation (Gradual Augmentation)\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, Rotate, RandomBrightnessContrast, GaussNoise, Blur, ShiftScaleRotate, \n",
    "    MotionBlur, Downscale, RandomFog, RandomShadow, Perspective\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_basic_augmentations():\n",
    "    # Lighter augmentations for initial training epochs\n",
    "    return Compose([\n",
    "        ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "        Blur(blur_limit=3, p=0.3),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_advanced_augmentations():\n",
    "    # Heavier augmentations for later epochs\n",
    "    return Compose([\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.7),\n",
    "        RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.7),\n",
    "        GaussNoise(var_limit=(10.0, 100.0), p=0.5),\n",
    "        Blur(blur_limit=5, p=0.5),\n",
    "        MotionBlur(blur_limit=5, p=0.5),\n",
    "        Downscale(scale_min=0.5, scale_max=0.9, p=0.4),\n",
    "        RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, alpha_coef=0.1, p=0.3),\n",
    "        RandomShadow(shadow_roi=(0,0,1,1), num_shadows_lower=1, num_shadows_upper=2, shadow_dimension=3, p=0.3),\n",
    "        Perspective(scale=(0.05,0.1), keep_size=True, p=0.5),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "class DocumentDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, split_file, processor, max_length=512, augmentations=None):\n",
    "        with open(split_file, 'r') as f:\n",
    "            self.filenames = [line.strip() for line in f]\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        # Will start with basic and later switch to advanced\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        image_path = os.path.join(self.images_dir, filename)\n",
    "        annotation_path = os.path.join(self.annotations_dir, f\"{os.path.splitext(filename)[0]}.json\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            annotation = json.load(f)\n",
    "        target_text = annotation['gt_text']['text_sequence']\n",
    "\n",
    "        if self.augmentations:\n",
    "            image_np = np.array(image)\n",
    "            augmented = self.augmentations(image=image_np)\n",
    "            image_tensor = augmented[\"image\"]\n",
    "        else:\n",
    "            image_tensor = None\n",
    "\n",
    "        if image_tensor is None:\n",
    "            encoding = self.processor(\n",
    "                images=image,\n",
    "                text=target_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "        else:\n",
    "            image_pil = Image.fromarray(image_tensor.permute(1,2,0).cpu().numpy().astype(np.uint8))\n",
    "            encoding = self.processor(\n",
    "                images=image_pil,\n",
    "                text=target_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "\n",
    "        labels = encoding.labels\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        return {\"pixel_values\": encoding.pixel_values.squeeze(0), \"labels\": labels.squeeze(0)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Callback: Gradual Augmentation\n",
    "\n",
    "class GradualAugmentationCallback(pl.Callback):\n",
    "    def __init__(self, dm, switch_epoch=10):\n",
    "        \"\"\"\n",
    "        dm: The DataModule instance.\n",
    "        switch_epoch: After this epoch, switch to advanced augmentations.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dm = dm\n",
    "        self.switch_epoch = switch_epoch\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        current_epoch = trainer.current_epoch\n",
    "        if current_epoch + 1 == self.switch_epoch:\n",
    "            print(f\"Switching to advanced augmentations at epoch {current_epoch+1}\")\n",
    "            self.dm.train_dataset.augmentations = get_advanced_augmentations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule (Initialization of Basic Augs)\n",
    "\n",
    "class DonutDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, processor,\n",
    "                 train_images_dir, train_annotations_dir, train_split_file,\n",
    "                 val_images_dir, val_annotations_dir, val_split_file,\n",
    "                 test_images_dir, test_annotations_dir, test_split_file,\n",
    "                 batch_size=4, num_workers=4, max_length=512):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.train_images_dir = train_images_dir\n",
    "        self.train_annotations_dir = train_annotations_dir\n",
    "        self.train_split_file = train_split_file\n",
    "        self.val_images_dir = val_images_dir\n",
    "        self.val_annotations_dir = val_annotations_dir\n",
    "        self.val_split_file = val_split_file\n",
    "        self.test_images_dir = test_images_dir\n",
    "        self.test_annotations_dir = test_annotations_dir\n",
    "        self.test_split_file = test_split_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Start with basic augmentations\n",
    "        self.train_augmentations = get_basic_augmentations()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = DocumentDataset(\n",
    "            self.train_images_dir, self.train_annotations_dir, self.train_split_file,\n",
    "            self.processor, self.max_length, augmentations=self.train_augmentations\n",
    "        )\n",
    "        self.val_dataset = DocumentDataset(\n",
    "            self.val_images_dir, self.val_annotations_dir, self.val_split_file,\n",
    "            self.processor, self.max_length, augmentations=None\n",
    "        )\n",
    "        self.test_dataset = DocumentDataset(\n",
    "            self.test_images_dir, self.test_annotations_dir, self.test_split_file,\n",
    "            self.processor, self.max_length, augmentations=None\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, collate_fn=collate_fn, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning Module (Adjusting configure_optimizers and add_special_tokens already done in dataset)\n",
    "\n",
    "class DonutModelModule(pl.LightningModule):\n",
    "    def __init__(self, processor, learning_rate=1e-5, weight_decay=0.01, num_training_steps=10000, warmup_steps=1000):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.processor = processor\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "        # Configuring special tokens\n",
    "        task_prompt = \"<s_ocr>\"\n",
    "        eos_token = \"</s>\"\n",
    "        special_tokens = {'additional_special_tokens': [task_prompt, eos_token]}\n",
    "        self.processor.tokenizer.add_special_tokens(special_tokens)\n",
    "        self.model.decoder.resize_token_embeddings(len(self.processor.tokenizer))\n",
    "        self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token\n",
    "\n",
    "        self.model.config.decoder_start_token_id = self.processor.tokenizer.convert_tokens_to_ids(task_prompt)\n",
    "        self.model.config.eos_token_id = self.processor.tokenizer.convert_tokens_to_ids(eos_token)\n",
    "        self.model.config.pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "\n",
    "        # Enabling gradient checkpointing\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        return self.model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "            val_loss = outputs.loss\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.hparams.num_training_steps\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Testing, and Saving\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "dm = DonutDataModule(\n",
    "    processor=processor,\n",
    "    train_images_dir=train_images_dir,\n",
    "    train_annotations_dir=train_annotations_dir,\n",
    "    train_split_file=train_split_file,\n",
    "    val_images_dir=val_images_dir,\n",
    "    val_annotations_dir=val_annotations_dir,\n",
    "    val_split_file=val_split_file,\n",
    "    test_images_dir=test_images_dir,\n",
    "    test_annotations_dir=test_annotations_dir,\n",
    "    test_split_file=test_split_file,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "steps_per_epoch = len(dm.train_dataloader())\n",
    "total_steps = steps_per_epoch * max_epochs\n",
    "warmup_steps = int(warmup_ratio * total_steps)\n",
    "\n",
    "model_module = DonutModelModule(\n",
    "    processor=processor,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    num_training_steps=total_steps,\n",
    "    warmup_steps=warmup_steps\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=save_dir,\n",
    "    filename=\"donut-best-checkpoint\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Gradual Augmentation Callback to switch at epoch 10\n",
    "gradual_callback = GradualAugmentationCallback(dm, switch_epoch=10)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=precision,\n",
    "    gradient_clip_val=1.0,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, gradual_callback],\n",
    "    log_every_n_steps=50\n",
    ")\n",
    "\n",
    "trainer.fit(model_module, datamodule=dm)\n",
    "\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at {best_model_path}\")\n",
    "model_module = DonutModelModule.load_from_checkpoint(best_model_path, processor=processor)\n",
    "model_module.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and Evaluation\n",
    "\n",
    "def compute_accuracies(predictions, references):\n",
    "    # Character-level accuracy\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "\n",
    "    # Word-level accuracy\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Character accuracy\n",
    "        pred_chars = list(pred)\n",
    "        ref_chars = list(ref)\n",
    "        total_chars += len(ref_chars)\n",
    "        correct_chars += sum(p == r for p, r in zip(pred_chars, ref_chars))\n",
    "\n",
    "        # Word accuracy\n",
    "        pred_words = pred.split()\n",
    "        ref_words = ref.split()\n",
    "        total_words += len(ref_words)\n",
    "        correct_words += sum(pw == rw for pw, rw in zip(pred_words, ref_words))\n",
    "\n",
    "    char_acc = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    word_acc = correct_words / total_words if total_words > 0 else 0\n",
    "    return char_acc, word_acc\n",
    "\n",
    "# Run inference on test set\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_module.to(device)\n",
    "\n",
    "model_module.model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model_module.model.generate(\n",
    "            pixel_values, \n",
    "            max_length=512,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=model_module.processor.tokenizer.convert_tokens_to_ids(\"<s_ocr>\")\n",
    "        )\n",
    "\n",
    "        batch_predictions = model_module.processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # Clean predictions (remove task prompt and eos token if needed)\n",
    "        batch_predictions = [\n",
    "            pred.replace(\"<s_ocr>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "            for pred in batch_predictions\n",
    "        ]\n",
    "\n",
    "        # Get references\n",
    "        for i, label in enumerate(labels):\n",
    "            # Convert labels back to text\n",
    "            text = model_module.processor.tokenizer.decode(\n",
    "                [l for l in label.tolist() if l != -100 and l != model_module.processor.tokenizer.pad_token_id], \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            text = text.replace(\"<s_ocr>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "            references.append(text)\n",
    "\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "char_acc, word_acc = compute_accuracies(predictions, references)\n",
    "\n",
    "print(\"Character-level Accuracy: {:.2f}%\".format(char_acc * 100))\n",
    "print(\"Word-level Accuracy: {:.2f}%\".format(word_acc * 100))\n",
    "\n",
    "# Print a few samples\n",
    "for i in range(min(3, len(predictions))):\n",
    "    print(\"Reference:\", references[i])\n",
    "    print(\"Prediction:\", predictions[i])\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and processor\n",
    "\n",
    "final_save_dir = os.path.join(data_root, \"final_donut2\")\n",
    "os.makedirs(final_save_dir, exist_ok=True)\n",
    "model_module.model.save_pretrained(final_save_dir)\n",
    "processor.save_pretrained(final_save_dir)\n",
    "\n",
    "print(\"Model and processor saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
